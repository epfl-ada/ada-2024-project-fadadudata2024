{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Axis : narrative themes and movie plots analysis\n",
    "\n",
    "In this axis, we aim at focusing on a more Natural Language Processing (NLP) strategy to process the CMU movie dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.lda_model\n",
    "from PIL import Image\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# we will need some ntlk dependencies\n",
    "nltk.download(['stopwords', 'wordnet', 'punkt_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Data and Plot Summaries ===\n",
    "DATASET_FILEPATH = Path(\"data/merged_movie_metadata.csv\")\n",
    "PLOT_SUMMARIES_FILEPATH = Path(\"data/plot_summaries.txt\")\n",
    "\n",
    "df = pd.read_csv(DATASET_FILEPATH)\n",
    "\n",
    "# Take only unique Wikipedia_ID\n",
    "df = df.drop_duplicates(subset=['Wikipedia_ID'])\n",
    "\n",
    "plot_summaries = {}\n",
    "\n",
    "with open(PLOT_SUMMARIES_FILEPATH, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t', 1)\n",
    "        if len(parts) == 2:\n",
    "            wiki_id, summary = parts\n",
    "            plot_summaries[int(wiki_id)] = summary\n",
    "\n",
    "# Map plot summaries to the DataFrame\n",
    "df['Plot_Summary_Base'] = df['Wikipedia_ID'].map(plot_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing Movie Plot Summaries\n",
    "\n",
    "Our dataset includes two main sources for movie plot summaries:\n",
    "\n",
    "1. **OMDB Data (Plot column)**: This source provides a brief summary of the movie plot in a few sentences.\n",
    "2. **CMU Movie Summary Corpus (plot_summaries.txt)**: This file, obtained from the [CMU Movie Summary Corpus website](https://www.cs.cmu.edu/~ark/personas/), contains detailed plot summaries for a range of movies.\n",
    "\n",
    "Since both sources may contain missing values for certain movies, we will combine the two to maximize coverage and ensure we have plot information for as many movies as possible. Below is a check to see how many values are missing in each plot source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in both plot sources\n",
    "print(f\"Total movies: {len(df)}\")\n",
    "print(f\"Missing values in OMDB Plot column: {df['Plot'].isna().sum()}\")\n",
    "print(f\"Missing values in CMU Plot Summary column: {df['Plot_Summary_Base'].isna().sum()}\")\n",
    "\n",
    "# Movies missing both plot summaries\n",
    "print(f\"Movies missing both summaries: {(df['Plot'].isna() & df['Plot_Summary_Base'].isna()).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additionnal dataset\n",
    "\n",
    "For our journey, we will need to retrieve an aditionnal dataset from the internet. We will get it from OMDB [Open Movie Database](https://www.omdbapi.com), an open API that allows to retrieve movie metadata from their IMDBId, or even simpler, from their title.\n",
    "\n",
    "After getting an API key, we managed to query the database for $73'000$ movies out of the $\\approx 83'000$ original movies of `movie_metadata.tsv` from CoreNLP.\n",
    "\n",
    "The resulting `.json` file was merged with the original CoreNLP dataset, adding the following columns :\n",
    "\n",
    "    [\"Rated\", \"Director\", \"Writer\", \"Actors\", \"Plot\", \"Awards\", \"Poster\", \"Ratings\", \"Metascore\", \"imdbRating\", \"imdbVotes\", \"imdbID\", \"DVD\", \"Production\", \"Website\", \"Response\", \"totalSeasons\", \"Oscar\", \"Nomination_Awards\", \"Win_Awards\", \"Internet_Movie_Database_Rating\", \"Rotten_Tomatoes_Rating\", \"Metacritic_Rating\"]\n",
    "\n",
    "We can then process to our text-based analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing for Movie Plots\n",
    "\n",
    "To analyze movie plots, we need to process the text data to standardize and clean it. We perform several key steps in this process:\n",
    "\n",
    "1. **Text selection** - For each movie, we prioritize plot summaries from the [CMU Movie Summary Corpus](https://www.cs.cmu.edu/~ark/personas/) because they tend to be richer in content. If a summary from this source is unavailable (i.e., a missing value), we use the summary from OMDB. If both sources are missing, the result will be marked as `NaN`.\n",
    "   \n",
    "2. **Tokenization** We split each plot summary into individual words (tokens).\n",
    "\n",
    "3. **Stop words removal** Commonly used words that don’t contribute meaningful information (known as stop words) are removed. Examples of stop words include \"the,\" \"is,\" \"and,\" \"in,\" \"to,\" etc.\n",
    "\n",
    "4. **Lemmatization** Each word is reduced to its base or root form. For instance, words like \"running,\" \"runs,\" and \"ran\" are all converted to \"run\". This helps improve the accuracy of our analysis by avoiding duplicates in different forms. This process is called.\n",
    "\n",
    "5. **Frequency analysis** We analyze the frequency and type of words used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Text Preprocessing ===\n",
    "from src.utils.data_utils import tok_by_region\n",
    "\n",
    "# Tokenize plots by region\n",
    "tokenized_plots_america = tok_by_region(df, 'America')\n",
    "tokenized_plots_europe = tok_by_region(df, 'Europe')\n",
    "tokenized_plots_both = tok_by_region(df, 'Both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to see the most frequently used words in movie plots across our three regions: **America**, **Europe**, and **Both**. We use a `Counter` to count occurrences of each word in the tokenized plots for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Word Counts ===\n",
    "from src.utils.data_utils import get_word_count\n",
    "\n",
    "word_count_america = get_word_count(tokenized_plots_america)\n",
    "word_count_europe = get_word_count(tokenized_plots_europe)\n",
    "word_count_both = get_word_count(tokenized_plots_both)\n",
    "\n",
    "print(\"Top 10 words in America:\", word_count_america.most_common(10))\n",
    "print(\"\\nTop 10 words in Europe:\", word_count_europe.most_common(10))\n",
    "print(\"\\nTop 10 words in Both:\", word_count_both.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the most frequently used words in each region’s plot summaries, but these results feel somewhat limited. To gain deeper insights, we’ll use the Stanford CoreNLP-processed summaries, which provide richer linguistic information.\n",
    "\n",
    "These summaries, derived from `plot_summaries.txt`, have been processed with the [Stanford CoreNLP pipeline](https://stanfordnlp.github.io/CoreNLP/), a tool that performs advanced language processing tasks such as part-of-speech tagging, syntactic parsing, named entity recognition (NER), and coreference resolution. This additional information will allow us to analyze not only word frequency but also the context and roles of words within each summary. We will first extract the **POS tags**.\n",
    "\n",
    "First, let's configure the folder where we store these summaries (not included in github due to the large space it takes) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(\"../corenlp_plot_summaries\")\n",
    "\n",
    "assert folder_path.exists(), \"Must configure the correct path to corenlp_plot_summaries\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's see how many movies that have their summaries processed.\n",
    "\n",
    " > **Note :** originally, only ~7'000 movies were processed. We added 3'500 missing movies by processing their OMDB plot summaries instead. This was a bit of a headache and since this is preprocessing, the logic was moved inside `src/scripts/` to lighten the notebook. See [extract_ids_to_run_in_pipeline.py](src/scripts/extract_ids_to_run_in_pipeline.py) and [INTELLIJ_run_Stanford_Pipeline.xml](src/scripts/INTELLIJ_run_Stanford_Pipeline.xml) for details on how this was achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matching folders/files\n",
    "processed_ids = {f.stem.replace(\".xml\", \"\") for f in folder_path.glob(\"*.xml.gz\")}\n",
    "available_ids = set(df[\"Wikipedia_ID\"].astype(str))\n",
    "matching_files = [folder_path/f\"{filename}.xml.gz\" for filename in processed_ids.intersection(available_ids)]\n",
    "\n",
    "print(f\"Number of matching folders/files: {len(matching_files)}\")\n",
    "print(f\"Number of unique Wiki id's: {df['Wikipedia_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter using the POS fields\n",
    "\n",
    "**POS (Part Of Speech) fields contains word metadata. It indicates whether the current word is a noun, a verb, an adjective, etc.**\n",
    "\n",
    "[An example POS definition can be found here](https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used) :\n",
    "\n",
    "![POS example definition](data/POS_tokens_BrownCorpusWikipedia.png)\n",
    "\n",
    "We need to know what kind of fields we have in our files to extract the most useful ones for our analysis. To do so, and to avoid unzipping all the xml files to the computer, we can leverage bash tools and use this (non-optimized) command. However, even if this method is (quiet) efficient, it would still take a long time to process all the files. Since we use this command only to get a grasp of the POS fields, we executed it on a subset of files chosen randomly. We just replace \"*xml.gz\" with the first 5000 files after being sorted randomly.\n",
    "\n",
    "\n",
    "```bash\n",
    "    zcat  $(ls *.xml.gz | sort -R | head -n5000) | grep -oP \"(?<=<POS>)[^<]+\" | sort | uniq -c | sort -rn\n",
    "```\n",
    "\n",
    "How it works :\n",
    " - `$(ls *.xml.gz | sort -R | head -n5000)` returns 5000 randomly chosen filenames inside the current directory.\n",
    " - `zcat` displays the content of a .gz (gzipped) file as text\n",
    " - The output is piped to `grep` which extracts all the POS fields as strings : \"(...) <POS>NN</POS> (...)\" => \"NN\" \n",
    " - The output is piped to `sort` which sorts this sequence of strings\n",
    " - The output is piped to `uniq` which prints each string only once, preceded with its number of occurences\n",
    " - The output is piped to `sort` which sorts the list of string occurences by descending order.\n",
    "\n",
    "\n",
    "Which gives as output :\n",
    "\n",
    "```text\n",
    "     250061 NN\n",
    "     185484 IN\n",
    "     164548 NNP\n",
    "     162675 DT\n",
    "     116598 VBZ\n",
    "      93384 ,\n",
    "     [...]\n",
    "```\n",
    "\n",
    "We redirect the output to a file `data/POS_tokens.csv` and read it with pandas :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv(\"data/POS_tokens.csv\", sep=\" \", header=None)\n",
    "df_pos.columns = [\"occurence\", \"token\"]\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_pos.plot(x='token', y='occurence', kind='bar', color='skyblue')\n",
    "plt.title(f'Repartition of the POS field content in 5000 files.', fontdict={'fontsize': 16, 'fontweight': 'bold'})\n",
    "plt.xlabel('POS', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.ylabel('Frequency', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a clear idea of the most commonly used POS tags, we can use this information to focus on the tags that will be most relevant and insightful for our analysis. \n",
    "Let's now see how many films in our dataset have gone through the Stanford CoreNLP pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the CoreNLP pipeline on the OMDB plots summaries (fallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of a total of 11'681 films, 11'319 have been processed through the Stanford CoreNLP pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the Stanford CoreNLP-processed data, we define the following functions:\n",
    "\n",
    "1. **`get_sentence_word_metadata`**:\n",
    "   - This function retrieves metadata from the CoreNLP pipeline output for a given movie ID. \n",
    "   - It extracts tokens, their lemmas, and part-of-speech (POS) tags from `.xml.gz` files.\n",
    "\n",
    "2. **`filter_words_by_pos`**:\n",
    "   - This function filters tokens based on specified POS tags, allowing us to focus on particular types of words, such as nouns, verbs, or adjectives.\n",
    "\n",
    "3. **`filter_words_by_pos_ngram`**:\n",
    "   - This function generates n-grams (sequences of n consecutive words) from the tokenized data, filtered by the specified POS tags. It is useful for capturing patterns and contexts in the text.\n",
    "\n",
    "We also define three sets of ids for each region (\"Europe\", \"America\" or both of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_utils import filter_words_by_pos, filter_words_by_pos_ngram, get_sentence_word_metadata\n",
    "\n",
    "# === Define Regions ===\n",
    "region_ids = {\n",
    "    \"America\": set(df[df['Continents'].str.contains(\"America\")]['Wikipedia_ID'].astype(str)),\n",
    "    \"Europe\": set(df[df['Continents'].str.contains(\"Europe\")]['Wikipedia_ID'].astype(str)),\n",
    "    \"Both\": set(df[df['Continents'].str.contains(\"Both\")]['Wikipedia_ID'].astype(str))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've successfully extracted the words we wanted, the next step is to visualize them. To do this, we will use a word cloud visualization to display the most frequent words. In a word cloud, word size represent their frequency of occurence. This gives immediately a grasp of the word occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Word Cloud Generation ===\n",
    "def generate_word_cloud(region, pos_tags, title, mask_image=None, sample_output=True, ngrams=1):\n",
    "    \"\"\"Generates a word cloud for a specified region and POS tags, with optional mask and sample output.\"\"\"\n",
    "    word_counter = Counter()\n",
    "    for wiki_id in tqdm(region_ids[region]):\n",
    "        tokens_metadata = get_sentence_word_metadata(folder_path, wiki_id)\n",
    "        filtered_words = filter_words_by_pos(tokens_metadata, pos_tags) if ngrams == 1 else filter_words_by_pos_ngram(tokens_metadata, pos_tags, ngrams)\n",
    "        word_counter.update(filtered_words)\n",
    "    \n",
    "    if sample_output:\n",
    "        # print sample output for verification\n",
    "        print(f\"\\nSample of filtered words for {region} - {title}:\")\n",
    "        print(word_counter.most_common(10))\n",
    "    \n",
    "    # word_counter.subtract(word_counter.most_common(6))\n",
    "    \n",
    "    # generate and display the word cloud with mask if provided\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', mask=mask_image,\n",
    "                          contour_color='black', contour_width=1).generate_from_frequencies(word_counter)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{title} Word Cloud for {region}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the POS tags defined above, we can group the tags into specific categories (e.g., nouns, verbs, adjectives) to filter tokens for our word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === POS Tags Groups ===\n",
    "noun_tags = ['NN', 'NNA', 'NNC', 'NNS', 'NNP', 'NNPS']\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBS', 'VBZ']\n",
    "adjective_tags = ['JJ', 'JJR', 'JJS', 'JJC', 'JJA', 'JJF', 'JJM']\n",
    "\n",
    "# === Word Cloud Mask Images ===\n",
    "americas_mask = np.array(Image.open(\"data/Location_North_America.png\"))\n",
    "europe_mask = np.array(Image.open(\"data/Location_Europe.png\"))\n",
    "both_mask = np.array(Image.open(\"data/Location_Both.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate Word Clouds ===\n",
    "\n",
    "generate_word_cloud(\"America\", noun_tags, \"Nouns in America\", mask_image=americas_mask, ngrams=2)\n",
    "generate_word_cloud(\"Europe\", verb_tags, \"Verbs in Europe\", mask_image=europe_mask, ngrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Experimental!] Applying Topic Modeling using LDA\n",
    "We aim to uncover the underlying themes in movie plot summaries from the **America**, **Europe**, and **Both** datasets using **Latent Dirichlet Allocation (LDA)**. This approach allows us to explore and compare thematic similarities and differences across regions. The methodology is inspired by this TDS article: [End-to-End Topic Modeling in Python](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocessed the plot data by tokenizing, lemmatizing, and removing stopwords. These cleaned tokens are joined into complete strings and combined into a single corpus (**America**, **Europe**, and **Both**) to create a shared vocabulary using the `TfidfVectorizer`.\n",
    "\n",
    "This shared vocabulary ensures consistent representation of words and phrases across all datasets. After fitting the vectorizer, we transform each region’s text into numerical representations (TF-IDF matrices). This allows us to analyze and compare themes and topics uniformly across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_america = tokenized_plots_america.dropna().apply(lambda tokens: ' '.join(tokens))\n",
    "documents_europe = tokenized_plots_europe.dropna().apply(lambda tokens: ' '.join(tokens))\n",
    "documents_both = tokenized_plots_both.dropna().apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "all_documents = pd.concat([documents_america, documents_europe, documents_both])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit vectorizer on the combined corpus\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    min_df=5,\n",
    "    max_df=0.85,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "vectorizer.fit(all_documents)\n",
    "\n",
    "# Transform each region separately\n",
    "tfidf_america = vectorizer.transform(documents_america)\n",
    "tfidf_europe = vectorizer.transform(documents_europe)\n",
    "tfidf_both = vectorizer.transform(documents_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit separate **Latent Dirichlet Allocation (LDA)** models for each region using their respective TF-IDF matrices. Each model identifies 3 latent topics (`n_components=3`) within the text data for its region. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LDA models\n",
    "lda_america = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "lda_america.fit(tfidf_america)\n",
    "\n",
    "lda_europe = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "lda_europe.fit(tfidf_europe)\n",
    "\n",
    "lda_both = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "lda_both.fit(tfidf_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `display_topics` function to extract and print the top words for each topic generated by the LDA models. \n",
    "\n",
    "This function:\n",
    "1. Loops through each topic in the model.\n",
    "2. Selects the top `no_top_words` most important words for the topic (based on their weights).\n",
    "3. Prints these words to summarize the theme of each topic.\n",
    "\n",
    "The `tfidf_feature_names`, which represent the vocabulary created by the `TfidfVectorizer`, are used to map the word indices back to their original terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[-no_top_words:]]))\n",
    "\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"America Topics:\")\n",
    "display_topics(lda_america, tfidf_feature_names)\n",
    "\n",
    "print(\"\\nEurope Topics:\")\n",
    "display_topics(lda_europe, tfidf_feature_names)\n",
    "\n",
    "print(\"\\nBoth Topics:\")\n",
    "display_topics(lda_both, tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are still somewhat messy, but we can infer some patterns. For example, **Topic 1** from the America dataset seemts to relate to crime and detective themes. However, the other topics are less clear and need refinement. This was just an initial experiment, and we aim to improve the results by tweaking the parameters and preprocessing steps in future iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyLDAvis Visualization\n",
    "\n",
    "1. **Intertopic Distance Map (Left Panel)**:\n",
    "   - Each circle represents a topic, and its size reflects how prevalent the topic is in the dataset.\n",
    "   - The distance between circles shows how distinct the topics are (closer = more similar, farther = more different).\n",
    "\n",
    "2. **Top-30 Relevant Terms (Right Panel)**:\n",
    "   - Displays the most relevant words for the selected topic.\n",
    "   - **Red bars**: Frequency of the word within the selected topic.\n",
    "   - **Blue bars**: Overall frequency of the word across the dataset.\n",
    "\n",
    "3. **λ Slider**:\n",
    "   - Adjusts the balance between specificity and frequency of terms.\n",
    "   - **λ = 1**: Shows terms unique to the topic.\n",
    "   - **λ = 0**: Shows more general terms that are common in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For America\n",
    "pyLDAvis.enable_notebook()\n",
    "vis_america = pyLDAvis.lda_model.prepare(lda_america, tfidf_america, vectorizer)\n",
    "pyLDAvis.display(vis_america)\n",
    "\n",
    "# For Europe\n",
    "vis_europe = pyLDAvis.lda_model.prepare(lda_europe, tfidf_europe, vectorizer)\n",
    "pyLDAvis.display(vis_europe)\n",
    "\n",
    "\n",
    "# For Both\n",
    "vis_both = pyLDAvis.lda_model.prepare(lda_both, tfidf_both, vectorizer)\n",
    "pyLDAvis.display(vis_both)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
