{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "nltk.download(['stopwords', 'wordnet', 'punkt_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Data and Plot Summaries ===\n",
    "df = pd.read_csv(\"data/merged_movie_metadata.csv\")\n",
    "\n",
    "# Take only unique Wikipedia_ID\n",
    "df = df.drop_duplicates(subset=['Wikipedia_ID'])\n",
    "\n",
    "plot_summaries_path = 'data/plot_summaries.txt'\n",
    "plot_summaries = {}\n",
    "\n",
    "with open(plot_summaries_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t', 1)\n",
    "        if len(parts) == 2:\n",
    "            wiki_id, summary = parts\n",
    "            plot_summaries[int(wiki_id)] = summary\n",
    "\n",
    "# Map plot summaries to the DataFrame\n",
    "df['Plot_Summary_Base'] = df['Wikipedia_ID'].map(plot_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing Movie Plot Summaries\n",
    "\n",
    "Our dataset includes two main sources for movie plot summaries:\n",
    "\n",
    "1. **OMDB Data (Plot column)**: This source provides a brief summary of the movie plot in a few sentences.\n",
    "2. **CMU Movie Summary Corpus (plot_summaries.txt)**: This file, obtained from the [CMU Movie Summary Corpus website](https://www.cs.cmu.edu/~ark/personas/), contains detailed plot summaries for a range of movies.\n",
    "\n",
    "Since both sources may contain missing values for certain movies, we will combine the two to maximize coverage and ensure we have plot information for as many movies as possible. Below is a check to see how many values are missing in each plot source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in both plot sources\n",
    "print(f\"Total movies: {len(df)}\")\n",
    "print(f\"Missing values in OMDB Plot column: {df['Plot'].isna().sum()}\")\n",
    "print(f\"Missing values in CMU Plot Summary column: {df['Plot_Summary_Base'].isna().sum()}\")\n",
    "\n",
    "# Movies missing both plot summaries\n",
    "print(f\"Movies missing both summaries: {(df['Plot'].isna() & df['Plot_Summary_Base'].isna()).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing for Movie Plots\n",
    "\n",
    "To analyze the language used in movie plots, we need to process the text data to standardize and clean it. We perform several key steps in this process:\n",
    "\n",
    "1. For each movie, we prioritize plot summaries from the [CMU Movie Summary Corpus](https://www.cs.cmu.edu/~ark/personas/) because they tend to be richer in content. If a summary from this source is unavailable (i.e., a missing value), we use the summary from OMDB. If both sources are missing, the result will be marked as `NaN`.\n",
    "   \n",
    "2. We split each plot summary into individual words (tokens), to analyze the frequency and type of words used.\n",
    "\n",
    "3. Commonly used words that don’t contribute meaningful information (known as stop words) are removed. Examples of stop words include \"the,\" \"is,\" \"and,\" \"in,\" \"to,\" etc.\n",
    "\n",
    "4. Each word is reduced to its base or root form. For instance, words like \"running,\" \"runs,\" and \"ran\" are all converted to \"run.\" This helps improve the accuracy of our analysis by avoiding duplicates in different forms. This process is called **lemmatization**.\n",
    "\n",
    "Below is the code that implements this preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Text Preprocessing ===\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tok(sentence):\n",
    "    \"\"\"Tokenize and lemmatize a sentence, removing stop words.\"\"\"\n",
    "    tokenized = word_tokenize(sentence)\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in tokenized if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "# Tokenize plots by region\n",
    "tokenized_plots_america = df[df['Continents'].str.contains('America')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "tokenized_plots_europe = df[df['Continents'].str.contains('Europe')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "tokenized_plots_both = df[df['Continents'].str.contains('Both')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to see the most frequently used words in movie plots across our three regions: **America**, **Europe**, and **Both**. We use a `Counter` to count occurrences of each word in the tokenized plots for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Word Counts ===\n",
    "def get_word_count(tokenized_plots):\n",
    "    \"\"\"Returns a Counter of words in tokenized plots.\"\"\"\n",
    "    return Counter(tokenized_plots.dropna().explode())\n",
    "\n",
    "word_count_america = get_word_count(tokenized_plots_america)\n",
    "word_count_europe = get_word_count(tokenized_plots_europe)\n",
    "word_count_both = get_word_count(tokenized_plots_both)\n",
    "\n",
    "print(\"Top 10 words in America:\", word_count_america.most_common(10))\n",
    "print(\"\\nTop 10 words in Europe:\", word_count_europe.most_common(10))\n",
    "print(\"\\nTop 10 words in Both:\", word_count_both.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the most frequently used words in each region’s plot summaries, but these results feel somewhat limited. To gain deeper insights, we’ll use the Stanford CoreNLP-processed summaries, which provide richer linguistic information.\n",
    "\n",
    "These summaries, derived from `plot_summaries.txt`, have been processed with the [Stanford CoreNLP pipeline](https://stanfordnlp.github.io/CoreNLP/), a tool that performs advanced language processing tasks such as part-of-speech tagging, syntactic parsing, named entity recognition (NER), and coreference resolution. This additional information will allow us to analyze not only word frequency but also the context and roles of words within each summary. We will first extract the **POS tags**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter using the POS fields\n",
    "\n",
    "**POS (Part Of Speech) fields contains word metadata. It indicates whether the current word is a noun, a verb, an adjective, etc.**\n",
    "\n",
    "[An example POS definition can be found here](https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used) :\n",
    "\n",
    "![POS example definition](data/POS_tokens_BrownCorpusWikipedia.png)\n",
    "\n",
    "We need to know what kind of fields we have in our files to extract the most useful ones for our analysis. To do so, and to avoid unzipping all the xml files to the computer, we can leverage bash tools and use this (non-optimized) command :\n",
    "\n",
    "```bash\n",
    "    zcat *.xml.gz | grep -oP \"(?<=<POS>)[^<]+\" | sort | uniq -c | sort -rn\n",
    "```\n",
    "\n",
    "How it works :\n",
    " - `zcat` displays the content of a .gz (gzipped) file as text\n",
    " - The output is piped to `grep` which extracts all the POS fields as strings : \"(...) <POS>NN</POS> (...)\" => \"NN\" \n",
    " - The output is piped to `sort` which sorts this sequence of strings\n",
    " - The output is piped to `uniq` which prints each string only once, preceded with its number of occurences\n",
    " - The output is piped to `sort` which sorts the list of string occurences by descending order.\n",
    "\n",
    "\n",
    "However, even if this method is (quiet) efficient, it would still take a long time to process all the files. Since we use this command only to get a grasp of the POS fields, we can execute it on a subset of files chosen randomly. We just replace \"*xml.gz\" with the first 5000 files after being sorted randomly.\n",
    "\n",
    "```bash\n",
    "    zcat  $(ls *.xml.gz | sort -R | head -n5000) | grep -oP \"(?<=<POS>)[^<]+\" | sort | uniq -c | sort -rn\n",
    "```\n",
    "\n",
    "Which gives as output :\n",
    "\n",
    "```text\n",
    "     250061 NN\n",
    "     185484 IN\n",
    "     164548 NNP\n",
    "     162675 DT\n",
    "     116598 VBZ\n",
    "      93384 ,\n",
    "      83306 JJ\n",
    "      78609 .\n",
    "      71877 PRP\n",
    "      64993 CC\n",
    "      59780 RB\n",
    "      58662 NNS\n",
    "      58193 TO\n",
    "      55793 VB\n",
    "      45140 PRP$\n",
    "      39655 VBN\n",
    "      37985 VBG\n",
    "      23029 VBP\n",
    "      18158 VBD\n",
    "      17188 POS\n",
    "      12434 CD\n",
    "      10615 RP\n",
    "      10395 WP\n",
    "       9718 WRB\n",
    "       7947 MD\n",
    "       5856 WDT\n",
    "       5332 ''\n",
    "       5325 ``\n",
    "       4710 :\n",
    "       3115 -LRB-\n",
    "       2315 JJR\n",
    "       2197 NNPS\n",
    "       1298 -RRB-\n",
    "       1211 JJS\n",
    "       1121 RBR\n",
    "        989 EX\n",
    "        775 PDT\n",
    "        696 FW\n",
    "        590 SYM\n",
    "        394 WP$\n",
    "        304 $\n",
    "        240 RBS\n",
    "        125 UH\n",
    "         36 #\n",
    "         17 LS\n",
    "```\n",
    "\n",
    "We redirect the output to a file `data/POS_tokens.csv` and read it with pandas :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv(\"data/POS_tokens.csv\", sep=\" \", header=None)\n",
    "df_pos.columns = [\"occurence\", \"token\"]\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_pos.plot(x='token', y='occurence', kind='bar', color='skyblue')\n",
    "plt.title(f'Repartition of the POS field content in 5000 files.', fontdict={'fontsize': 16, 'fontweight': 'bold'})\n",
    "plt.xlabel('POS', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.ylabel('Frequency', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper Functions for Metadata Extraction ===\n",
    "\n",
    "folder_path = Path(\"../corenlp_plot_summaries\")\n",
    "\n",
    "assert folder_path.exists(), \"Must configure the correct path to corenlp_plot_summaries\"\n",
    "\n",
    "# count matching folders/files\n",
    "processed_ids = {f.stem.replace(\".xml\", \"\") for f in folder_path.glob(\"*.xml.gz\")}\n",
    "available_ids = set(df[\"Wikipedia_ID\"].astype(str))\n",
    "matching_files = [folder_path/f\"{filename}.xml.gz\" for filename in processed_ids.intersection(available_ids)]\n",
    "\n",
    "print(f\"Number of matching folders/files: {len(matching_files)}\")\n",
    "print(f\"Number of unique Wiki id's: {df['Wikipedia_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7605 out of 11681 films that have went through the Stanford pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_word_metadata(id, print_output=False):\n",
    "    \"\"\"Retrieve Stanford CoreNLP metadata for a given ID from .xml.gz files.\"\"\"\n",
    "    gz_file_path = folder_path / f\"{id}.xml.gz\"\n",
    "    if not gz_file_path.is_file():\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        with gzip.open(gz_file_path, 'rb') as gz_file:\n",
    "            xml_data = gz_file.read()\n",
    "            root = ET.fromstring(xml_data.decode())\n",
    "        \n",
    "        summary_word_metadata = []\n",
    "        for sentence in root[0][0]:\n",
    "            if print_output:\n",
    "                print(f\"\\n\\n=================== Sentence n°{sentence.attrib['id']} ===================\")\n",
    "            for child in sentence:\n",
    "                if child.tag != \"tokens\":\n",
    "                    continue\n",
    "                for token in child:\n",
    "                    attribs = {c.tag: c.text for c in token}\n",
    "                    if print_output:\n",
    "                        print(f\"{attribs['word']} ({attribs['lemma']}) => {attribs['POS']}\")\n",
    "                    summary_word_metadata.append(attribs)\n",
    "        return summary_word_metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {gz_file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "# === Filter Functions ===\n",
    "def filter_words_by_pos(tokens_metadata, pos_tags):\n",
    "    \"\"\"Filter words by specified POS tags.\"\"\"\n",
    "    return [entry['word'] for entry in tokens_metadata if entry['POS'] in pos_tags]\n",
    "\n",
    "def filter_words_by_pos_ngram(tokens_metadata, pos_tags, ngram):\n",
    "    \"\"\"Generate n-grams filtered by specified POS tags.\"\"\"\n",
    "    results = []\n",
    "    for ngram_instance in nltk.ngrams(tokens_metadata, ngram):\n",
    "        if ngram_instance[0]['POS'] in pos_tags:\n",
    "            results.append(' '.join(entry['word'] for entry in ngram_instance))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Word Cloud Generation ===\n",
    "def generate_word_cloud(region, pos_tags, title, mask_image=None, sample_output=True, ngrams=1):\n",
    "    \"\"\"Generates a word cloud for a specified region and POS tags, with optional mask and sample output.\"\"\"\n",
    "    word_counter = Counter()\n",
    "    for wiki_id in tqdm(region_ids[region]):\n",
    "        tokens_metadata = get_sentence_word_metadata(wiki_id)\n",
    "        filtered_words = filter_words_by_pos(tokens_metadata, pos_tags) if ngrams == 1 else filter_words_by_pos_ngram(tokens_metadata, pos_tags, ngrams)\n",
    "        word_counter.update(filtered_words)\n",
    "    \n",
    "    if sample_output:\n",
    "        # print sample output for verification\n",
    "        print(f\"\\nSample of filtered words for {region} - {title}:\")\n",
    "        print(word_counter.most_common(10))\n",
    "    \n",
    "    # generate and display the word cloud with mask if provided\n",
    "    \n",
    "    # TODO omit the most common ?\n",
    "    # word_counter.subtract(word_counter.most_common(6))\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', mask=mask_image,\n",
    "                          contour_color='black', contour_width=1).generate_from_frequencies(word_counter)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{title} Word Cloud for {region}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above POS denomination, we can group the fields we want to appear in a list so that only tokens that match those POS appear in our wordcloud :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Regions ===\n",
    "region_ids = {\n",
    "    \"America\": set(df[df['Continents'].str.contains(\"America\")]['Wikipedia_ID'].astype(str)),\n",
    "    \"Europe\": set(df[df['Continents'].str.contains(\"Europe\")]['Wikipedia_ID'].astype(str)),\n",
    "    \"Both\": set(df[df['Continents'].str.contains(\"Both\")]['Wikipedia_ID'].astype(str))\n",
    "}\n",
    "\n",
    "# === POS Tags Groups ===\n",
    "noun_tags = ['NN', 'NNA', 'NNC', 'NNS', 'NNP', 'NNPS']\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBS', 'VBZ']\n",
    "adjective_tags = ['JJ', 'JJR', 'JJS', 'JJC', 'JJA', 'JJF', 'JJM']\n",
    "\n",
    "# === Word Cloud Mask Images ===\n",
    "americas_mask = np.array(Image.open(\"data/Location_North_America.png\"))\n",
    "europe_mask = np.array(Image.open(\"data/Location_Europe.png\"))\n",
    "both_mask = np.array(Image.open(\"data/Location_Both.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate Word Clouds ===\n",
    "\n",
    "# Generate example word clouds\n",
    "generate_word_cloud(\"America\", noun_tags, \"Nouns in America\", mask_image=americas_mask, ngrams=2)\n",
    "generate_word_cloud(\"Europe\", verb_tags, \"Verbs in Europe\", mask_image=europe_mask, ngrams=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
