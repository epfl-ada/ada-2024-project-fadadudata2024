{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/merged_movie_metadata.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summaries_path = 'data/plot_summaries.txt'\n",
    "plot_summaries = {}\n",
    "with open(plot_summaries_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t', 1)\n",
    "        if len(parts) == 2:\n",
    "            wiki_id, summary = parts\n",
    "            plot_summaries[int(wiki_id)] = summary\n",
    "            \n",
    "\n",
    "# add plot_summaries.txt\n",
    "df['Plot_Summary_Base'] = df['Wikipedia_ID'].map(plot_summaries)\n",
    "\n",
    "# take only unique Wikipedia_ID\n",
    "df['Wikipedia_ID'].nunique()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(['stopwords', 'wordnet', 'punkt_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tok(sentence):\n",
    "    tokenized = word_tokenize(sentence)\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in tokenized if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_america = df[df['Continents'].str.contains('America')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_europe = df[df['Continents'].str.contains('Europe')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_both = df[df['Continents'].str.contains('Both')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count_america = Counter(tokenized_plots_america.dropna().explode())\n",
    "word_count_europe = Counter(tokenized_plots_europe.dropna().explode())\n",
    "word_count_both = Counter(tokenized_plots_both.dropna().explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 words in America:\")\n",
    "print(word_count_america.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 words in Europe:\")\n",
    "print(word_count_europe.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 words in Both:\")\n",
    "print(word_count_both.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def filter_stop_words(word_list):\n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "filtered_word_count_america = Counter(filter_stop_words(tokenized_plots_america.dropna().explode()))\n",
    "filtered_word_count_europe = Counter(filter_stop_words(tokenized_plots_europe.dropna().explode()))\n",
    "filtered_word_count_both = Counter(filter_stop_words(tokenized_plots_both.dropna().explode()))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in America:\")\n",
    "print(filtered_word_count_america.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in Europe:\")\n",
    "print(filtered_word_count_europe.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in Both:\")\n",
    "print(filtered_word_count_both.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "americas_mask = np.array(Image.open(\"data/Location_North_America.png\"))\n",
    "europe_mask = np.array(Image.open(\"data/Location_Europe.png\"))\n",
    "both_mask = np.array(Image.open(\"data/Location_Both.png\"))\n",
    "\n",
    "\n",
    "def generate_word_cloud(word_count, title, mask_image):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                          mask=mask_image).generate_from_frequencies(word_count)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_word_cloud(filtered_word_count_america, \"Word Cloud for America\", americas_mask)\n",
    "generate_word_cloud(filtered_word_count_europe, \"Word Cloud for Europe\", europe_mask)\n",
    "generate_word_cloud(filtered_word_count_both, \"Word Cloud for Both\", both_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter using the POS fields\n",
    "\n",
    "**POS (Part Of Speech) fields contains word metadata. It indicates whether the current word is a noun, a verb, an adjective, etc.**\n",
    "\n",
    "[An example POS definition can be found here](https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used) :\n",
    "\n",
    "![POS example definition](data/POS_tokens_BrownCorpusWikipedia.png)\n",
    "\n",
    "We need to know what kind of fields we have in our files to extract the most useful ones for our analysis. To do so, and to avoid unzipping all the xml files to the computer, we can leverage bash tools and use this (non-optimized) command :\n",
    "\n",
    "```bash\n",
    "    zcat *.xml.gz | grep -oP \"(?<=<POS>)[^<]+\" | sort | uniq -c | sort -rn\n",
    "```\n",
    "\n",
    "How it works :\n",
    " - `zcat` displays the content of a .gz (gzipped) file as text\n",
    " - The output is piped to `grep` which extracts all the POS fields as strings : \"(...) <POS>NN</POS> (...)\" => \"NN\" \n",
    " - The output is piped to `sort` which sorts this sequence of strings\n",
    " - The output is piped to `uniq` which prints each string only once, preceded with its number of occurences\n",
    " - The output is piped to `sort` which sorts the list of string occurences by descending order.\n",
    "\n",
    "\n",
    "However, even if this method is (quiet) efficient, it would still take a long time to process all the files. Since we use this command only to get a grasp of the POS fields, we can execute it on a subset of files chosen randomly. We just replace \"*xml.gz\" with the first 5000 files after being sorted randomly.\n",
    "\n",
    "```bash\n",
    "    zcat  $(ls *.xml.gz | sort -R | head -n5000) | grep -oP \"(?<=<POS>)[^<]+\" | sort | uniq -c | sort -rn\n",
    "```\n",
    "\n",
    "Which gives as output :\n",
    "\n",
    "```text\n",
    "     250061 NN\n",
    "     185484 IN\n",
    "     164548 NNP\n",
    "     162675 DT\n",
    "     116598 VBZ\n",
    "      93384 ,\n",
    "      83306 JJ\n",
    "      78609 .\n",
    "      71877 PRP\n",
    "      64993 CC\n",
    "      59780 RB\n",
    "      58662 NNS\n",
    "      58193 TO\n",
    "      55793 VB\n",
    "      45140 PRP$\n",
    "      39655 VBN\n",
    "      37985 VBG\n",
    "      23029 VBP\n",
    "      18158 VBD\n",
    "      17188 POS\n",
    "      12434 CD\n",
    "      10615 RP\n",
    "      10395 WP\n",
    "       9718 WRB\n",
    "       7947 MD\n",
    "       5856 WDT\n",
    "       5332 ''\n",
    "       5325 ``\n",
    "       4710 :\n",
    "       3115 -LRB-\n",
    "       2315 JJR\n",
    "       2197 NNPS\n",
    "       1298 -RRB-\n",
    "       1211 JJS\n",
    "       1121 RBR\n",
    "        989 EX\n",
    "        775 PDT\n",
    "        696 FW\n",
    "        590 SYM\n",
    "        394 WP$\n",
    "        304 $\n",
    "        240 RBS\n",
    "        125 UH\n",
    "         36 #\n",
    "         17 LS\n",
    "```\n",
    "\n",
    "We redirect the output to a file `data/POS_tokens.csv` and read it with pandas :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv(\"data/POS_tokens.csv\", sep=\" \", header=None)\n",
    "df_pos.columns = [\"occurence\", \"token\"]\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_pos.plot(x='token', y='occurence', kind='bar', color='skyblue')\n",
    "plt.title(f'Repartition of the POS field content in 5000 files.', fontdict={'fontsize': 16, 'fontweight': 'bold'})\n",
    "plt.xlabel('POS', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.ylabel('Frequency', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the compressed files are\n",
    "folder_path = Path(\"../corenlp_plot_summaries\")\n",
    "\n",
    "assert folder_path.exists(), \"Must configure the correct path to corenlp_plot_summaries\"\n",
    "\n",
    "# count matching folders/files\n",
    "processed_ids = {f.stem.replace(\".xml\", \"\") for f in folder_path.glob(\"*.xml.gz\")}\n",
    "available_ids = set(df[\"Wikipedia_ID\"].astype(str))\n",
    "matching_files = [folder_path/f\"{filename}.xml.gz\" for filename in processed_ids.intersection(available_ids)]\n",
    "\n",
    "print(f\"Number of matching folders/files: {len(matching_files)}\")\n",
    "print(f\"Number of unique Wiki id's: {df['Wikipedia_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7605 out of 11681 films that have went through the Stanford pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_word_metadata(id, print_output=False):\n",
    "    \"\"\"Returns the output of the Stanford pipeline for param id in a list of sentences (=list of dict).\n",
    "       If print_output is True, prints the token details.\n",
    "    \"\"\"\n",
    "    gz_file_path = folder_path / f\"{id}.xml.gz\"\n",
    "\n",
    "    if not gz_file_path.is_file():\n",
    "        #print(f\"Unable to find the corresponding .gz file for {id}.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # open and read the .gz file, then parse the XML content\n",
    "        with gzip.open(gz_file_path, 'rb') as gz_file:\n",
    "            xml_data = gz_file.read()\n",
    "            root = ET.fromstring(xml_data.decode())  # parse XML from the string\n",
    "\n",
    "        # process sentences and tokens as in the original code\n",
    "        sentences = root[0][0]\n",
    "        summary_word_metadata = []\n",
    "        for sentence in sentences:\n",
    "            if print_output:\n",
    "                print(f\"\\n\\n====================== Sentence n°{sentence.attrib['id']} ======================\")\n",
    "            for child in sentence:\n",
    "                if child.tag != \"tokens\":\n",
    "                    continue\n",
    "                for token in child:\n",
    "                    attribs = {c.tag: c.text for c in token}\n",
    "                    if print_output:\n",
    "                        print(f\"{attribs['word']} ({attribs['lemma']}) => {attribs['POS']}\")\n",
    "                    summary_word_metadata.append(attribs)\n",
    "        return summary_word_metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {gz_file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def filter_words_by_pos(tokens_metadata, pos_tags):\n",
    "    \"\"\"Filter words from tokens_metadata by specified POS tags.\"\"\"\n",
    "    return [entry['word'] for entry in tokens_metadata if entry['POS'] in pos_tags]\n",
    "\n",
    "def filter_words_by_pos_ngram(tokens_metadata, pos_tags, ngram):\n",
    "    \"\"\"Filter ngrams from tokens_metadata by specified POS tags.\"\"\"\n",
    "    results = []\n",
    "    for ngram_instance in nltk.ngrams(tokens_metadata, ngram):\n",
    "        if ngram_instance[0]['POS'] in pos_tags:\n",
    "            results.append((' ').join([entry['word'] for entry in ngram_instance]))\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_word_cloud(region, pos_tags, title, mask_image=None, sample_output=True, ngrams=1):\n",
    "    \"\"\"Generates a word cloud for a specified region and POS tags, with optional mask and sample output.\"\"\"\n",
    "    word_counter = Counter()\n",
    "    for wiki_id in tqdm(region_ids[region]):\n",
    "        tokens_metadata = get_sentence_word_metadata(wiki_id)\n",
    "        filtered_words = filter_words_by_pos(tokens_metadata, pos_tags) if ngrams == 1 else filter_words_by_pos_ngram(tokens_metadata, pos_tags, ngrams)\n",
    "        word_counter.update(filtered_words)\n",
    "    \n",
    "    if sample_output:\n",
    "        # print sample output for verification\n",
    "        print(f\"\\nSample of filtered words for {region} - {title}:\")\n",
    "        print(word_counter.most_common(10))\n",
    "    \n",
    "    # generate and display the word cloud with mask if provided\n",
    "    \n",
    "    # TODO omit the most common ?\n",
    "    # word_counter.subtract(word_counter.most_common(6))\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', mask=mask_image,\n",
    "                          contour_color='black', contour_width=1).generate_from_frequencies(word_counter)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{title} Word Cloud for {region}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above POS denomination, we can group the fields we want to appear in a list so that only tokens that math those POS appear in our wordcloud :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "\n",
    "# get IDs by region\n",
    "region_ids = {\n",
    "    \"America\": set(df[df['Continents'].str.contains(\"America\")]['Wikipedia_ID'].astype(str)),\n",
    "    \"Europe\": set(df[df['Continents'].str.contains(\"Europe\")]['Wikipedia_ID'].astype(str)),\n",
    "    \"Both\": set(df[df['Continents'].str.contains(\"Both\")]['Wikipedia_ID'].astype(str))\n",
    "}\n",
    "\n",
    "# POS tags for nouns or verbs\n",
    "# !! à completer\n",
    "noun_tags = ['NN', 'NNA', 'NNC', 'NNS', 'NNP', 'NNPS']\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBS', 'VBZ']\n",
    "adjective_tags = ['JJ', 'JJR', 'JJS', 'JJC', 'JJA', 'JJF', 'JJM']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_word_cloud(\"America\", noun_tags, \"Nouns in America\", mask_image=americas_mask, ngrams=2)\n",
    "generate_word_cloud(\"Europe\", verb_tags, \"Verbs in Europe\", mask_image=europe_mask, ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud(\"America\", noun_tags, \"Nouns in America\", mask_image=americas_mask)\n",
    "generate_word_cloud(\"Europe\", noun_tags, \"Nouns in Europe\", mask_image=europe_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud(\"America\", verb_tags, \"Verbs in America\", mask_image=americas_mask)\n",
    "generate_word_cloud(\"Europe\", verb_tags, \"Verbs in Europe\", mask_image=europe_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud(\"America\", adjective_tags, \"Adjectives in America\", mask_image=americas_mask)\n",
    "generate_word_cloud(\"Europe\", adjective_tags, \"Adjectives in Europe\", mask_image=europe_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
