{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/merged_movie_metadata.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summaries_path = 'data/plot_summaries.txt'\n",
    "plot_summaries = {}\n",
    "with open(plot_summaries_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t', 1)\n",
    "        if len(parts) == 2:\n",
    "            wiki_id, summary = parts\n",
    "            plot_summaries[int(wiki_id)] = summary\n",
    "            \n",
    "\n",
    "# add plot_summaries.txt\n",
    "df['Plot_Summary_Base'] = df['Wikipedia_ID'].map(plot_summaries)\n",
    "\n",
    "# take only unique Wikipedia_ID\n",
    "df['Wikipedia_ID'].nunique()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tok(sentence):\n",
    "    tokenized = word_tokenize(sentence)\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in tokenized if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_america = df[df['Continents'].str.contains('America')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_europe = df[df['Continents'].str.contains('Europe')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_both = df[df['Continents'].str.contains('Both')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count_america = Counter(tokenized_plots_america.dropna().explode())\n",
    "word_count_europe = Counter(tokenized_plots_europe.dropna().explode())\n",
    "word_count_both = Counter(tokenized_plots_both.dropna().explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 words in America:\")\n",
    "print(word_count_america.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 words in Europe:\")\n",
    "print(word_count_europe.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 words in Both:\")\n",
    "print(word_count_both.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def filter_stop_words(word_list):\n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "filtered_word_count_america = Counter(filter_stop_words(tokenized_plots_america.dropna().explode()))\n",
    "filtered_word_count_europe = Counter(filter_stop_words(tokenized_plots_europe.dropna().explode()))\n",
    "filtered_word_count_both = Counter(filter_stop_words(tokenized_plots_both.dropna().explode()))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in America:\")\n",
    "print(filtered_word_count_america.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in Europe:\")\n",
    "print(filtered_word_count_europe.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in Both:\")\n",
    "print(filtered_word_count_both.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_word_cloud(word_count, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_count)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_word_cloud(filtered_word_count_america, \"Word Cloud for America\")\n",
    "generate_word_cloud(filtered_word_count_europe, \"Word Cloud for Europe\")\n",
    "generate_word_cloud(filtered_word_count_both, \"Word Cloud for Both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Do:\n",
    "- Filtrer encore parce que les mots ne sont pas hyper intéressants\n",
    "- Faire le word cloud en forme des continents (j'ai essayé mais c'était affreux)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
