{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/merged_movie_metadata.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summaries_path = 'data/plot_summaries.txt'\n",
    "plot_summaries = {}\n",
    "with open(plot_summaries_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t', 1)\n",
    "        if len(parts) == 2:\n",
    "            wiki_id, summary = parts\n",
    "            plot_summaries[int(wiki_id)] = summary\n",
    "            \n",
    "\n",
    "# add plot_summaries.txt\n",
    "df['Plot_Summary_Base'] = df['Wikipedia_ID'].map(plot_summaries)\n",
    "\n",
    "# take only unique Wikipedia_ID\n",
    "df['Wikipedia_ID'].nunique()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(['stopwords', 'wordnet', 'punkt_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tok(sentence):\n",
    "    tokenized = word_tokenize(sentence)\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in tokenized if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_america = df[df['Continents'].str.contains('America')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_europe = df[df['Continents'].str.contains('Europe')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_both = df[df['Continents'].str.contains('Both')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count_america = Counter(tokenized_plots_america.dropna().explode())\n",
    "word_count_europe = Counter(tokenized_plots_europe.dropna().explode())\n",
    "word_count_both = Counter(tokenized_plots_both.dropna().explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 words in America:\")\n",
    "print(word_count_america.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 words in Europe:\")\n",
    "print(word_count_europe.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 words in Both:\")\n",
    "print(word_count_both.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def filter_stop_words(word_list):\n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "filtered_word_count_america = Counter(filter_stop_words(tokenized_plots_america.dropna().explode()))\n",
    "filtered_word_count_europe = Counter(filter_stop_words(tokenized_plots_europe.dropna().explode()))\n",
    "filtered_word_count_both = Counter(filter_stop_words(tokenized_plots_both.dropna().explode()))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in America:\")\n",
    "print(filtered_word_count_america.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in Europe:\")\n",
    "print(filtered_word_count_europe.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in Both:\")\n",
    "print(filtered_word_count_both.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "americas_mask = np.array(Image.open(\"data/Location_North_America.png\"))\n",
    "europe_mask = np.array(Image.open(\"data/Location_Europe.png\"))\n",
    "both_mask = np.array(Image.open(\"data/Location_Both.png\"))\n",
    "\n",
    "\n",
    "def generate_word_cloud(word_count, title, mask_image):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                          mask=mask_image).generate_from_frequencies(word_count)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_word_cloud(filtered_word_count_america, \"Word Cloud for America\", americas_mask)\n",
    "generate_word_cloud(filtered_word_count_europe, \"Word Cloud for Europe\", europe_mask)\n",
    "generate_word_cloud(filtered_word_count_both, \"Word Cloud for Both\", both_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_word_metadata(id):\n",
    "    \"\"\"Returns the output of the Stanford pipeline for param id in a list of sentences (=list of dict)\n",
    "    \"\"\"\n",
    "    filename = Path(f\"../corenlp_plot_summaries/{id}.xml\")\n",
    "    if not filename.exists():\n",
    "        print(f\"Unable to find the corresponding file for {id}.\")\n",
    "        return []\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    sentences = root[0][0]\n",
    "    summary_word_metadata = []\n",
    "    for sentence in sentences:\n",
    "        print(f\"\\n\\n====================== Sentence n°{sentence.attrib['id']} ======================\")\n",
    "        for child in sentence:\n",
    "            if child.tag != \"tokens\":\n",
    "                continue\n",
    "            for token in child:\n",
    "                attribs = {c.tag: c.text for c in token}\n",
    "                print(f\"{attribs['word']} ({attribs['lemma']}) => {attribs['POS']}\")\n",
    "                summary_word_metadata.append(attribs)\n",
    "    return summary_word_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence_word_metadata(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Do:\n",
    "- Filtrer encore parce que les mots ne sont pas hyper intéressants\n",
    "- Faire le word cloud en forme des continents (j'ai essayé mais c'était affreux)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
