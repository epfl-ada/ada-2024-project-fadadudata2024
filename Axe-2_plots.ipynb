{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/merged_movie_metadata.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summaries_path = 'data/plot_summaries.txt'\n",
    "plot_summaries = {}\n",
    "with open(plot_summaries_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t', 1)\n",
    "        if len(parts) == 2:\n",
    "            wiki_id, summary = parts\n",
    "            plot_summaries[int(wiki_id)] = summary\n",
    "            \n",
    "\n",
    "# add plot_summaries.txt\n",
    "df['Plot_Summary_Base'] = df['Wikipedia_ID'].map(plot_summaries)\n",
    "\n",
    "# take only unique Wikipedia_ID\n",
    "df['Wikipedia_ID'].nunique()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(['stopwords', 'wordnet', 'punkt_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tok(sentence):\n",
    "    tokenized = word_tokenize(sentence)\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in tokenized if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_america = df[df['Continents'].str.contains('America')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_europe = df[df['Continents'].str.contains('Europe')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_plots_both = df[df['Continents'].str.contains('Both')].apply(\n",
    "    lambda row: tok(row['Plot_Summary_Base']) if pd.notna(row['Plot_Summary_Base']) \n",
    "    else tok(row['Plot']) if pd.notna(row['Plot']) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count_america = Counter(tokenized_plots_america.dropna().explode())\n",
    "word_count_europe = Counter(tokenized_plots_europe.dropna().explode())\n",
    "word_count_both = Counter(tokenized_plots_both.dropna().explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 words in America:\")\n",
    "print(word_count_america.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 words in Europe:\")\n",
    "print(word_count_europe.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 words in Both:\")\n",
    "print(word_count_both.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def filter_stop_words(word_list):\n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "filtered_word_count_america = Counter(filter_stop_words(tokenized_plots_america.dropna().explode()))\n",
    "filtered_word_count_europe = Counter(filter_stop_words(tokenized_plots_europe.dropna().explode()))\n",
    "filtered_word_count_both = Counter(filter_stop_words(tokenized_plots_both.dropna().explode()))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in America:\")\n",
    "print(filtered_word_count_america.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in Europe:\")\n",
    "print(filtered_word_count_europe.most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 interesting words in Both:\")\n",
    "print(filtered_word_count_both.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "americas_mask = np.array(Image.open(\"data/Location_North_America.png\"))\n",
    "europe_mask = np.array(Image.open(\"data/Location_Europe.png\"))\n",
    "both_mask = np.array(Image.open(\"data/Location_Both.png\"))\n",
    "\n",
    "\n",
    "def generate_word_cloud(word_count, title, mask_image):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                          mask=mask_image).generate_from_frequencies(word_count)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generate_word_cloud(filtered_word_count_america, \"Word Cloud for America\", americas_mask)\n",
    "generate_word_cloud(filtered_word_count_europe, \"Word Cloud for Europe\", europe_mask)\n",
    "generate_word_cloud(filtered_word_count_both, \"Word Cloud for Both\", both_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the compressed files are\n",
    "folder_path = Path(\"your_path/corenlp_plot_summaries\")\n",
    "\n",
    "# count matching folders/files\n",
    "matching_files = [file for file in folder_path.glob(\"*.xml.gz\") if file.stem.replace('.xml', '') in set(df['Wikipedia_ID'].astype(str))]\n",
    "\n",
    "\n",
    "print(f\"Number of matching folders/files: {len(matching_files)}\")\n",
    "print(f\"Number of unique Wiki id's: {df['Wikipedia_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7605 out of 11681 films that have went through the Stanford pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_word_metadata(id, print_output=False):\n",
    "    \"\"\"Returns the output of the Stanford pipeline for param id in a list of sentences (=list of dict).\n",
    "       If print_output is True, prints the token details.\n",
    "    \"\"\"\n",
    "    gz_file_path = folder_path / f\"{id}.xml.gz\"\n",
    "\n",
    "    if not gz_file_path.is_file():\n",
    "        #print(f\"Unable to find the corresponding .gz file for {id}.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # open and read the .gz file, then parse the XML content\n",
    "        with gzip.open(gz_file_path, 'rb') as gz_file:\n",
    "            xml_data = gz_file.read()\n",
    "            root = ET.fromstring(xml_data.decode())  # parse XML from the string\n",
    "\n",
    "        # process sentences and tokens as in the original code\n",
    "        sentences = root[0][0]\n",
    "        summary_word_metadata = []\n",
    "        for sentence in sentences:\n",
    "            if print_output:\n",
    "                print(f\"\\n\\n====================== Sentence nÂ°{sentence.attrib['id']} ======================\")\n",
    "            for child in sentence:\n",
    "                if child.tag != \"tokens\":\n",
    "                    continue\n",
    "                for token in child:\n",
    "                    attribs = {c.tag: c.text for c in token}\n",
    "                    if print_output:\n",
    "                        print(f\"{attribs['word']} ({attribs['lemma']}) => {attribs['POS']}\")\n",
    "                    summary_word_metadata.append(attribs)\n",
    "        return summary_word_metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {gz_file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def filter_words_by_pos(tokens_metadata, pos_tags):\n",
    "    \"\"\"Filter words from tokens_metadata by specified POS tags.\"\"\"\n",
    "    return [entry['word'] for entry in tokens_metadata if entry['POS'] in pos_tags]\n",
    "\n",
    "\n",
    "def generate_word_cloud(region, pos_tags, title, mask_image=None, sample_output=True):\n",
    "    \"\"\"Generates a word cloud for a specified region and POS tags, with optional mask and sample output.\"\"\"\n",
    "    word_counter = Counter()\n",
    "    for wiki_id in region_ids[region]:\n",
    "        tokens_metadata = get_sentence_word_metadata(wiki_id)\n",
    "        filtered_words = filter_words_by_pos(tokens_metadata, pos_tags)\n",
    "        word_counter.update(filtered_words)\n",
    "    \n",
    "    if sample_output:\n",
    "        # print sample output for verification\n",
    "        print(f\"\\nSample of filtered words for {region} - {title}:\")\n",
    "        print(word_counter.most_common(10))\n",
    "    \n",
    "    # generate and display the word cloud with mask if provided\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', mask=mask_image,\n",
    "                          contour_color='black', contour_width=1).generate_from_frequencies(word_counter)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{title} Word Cloud for {region}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "\n",
    "# get IDs by region\n",
    "region_ids = {\n",
    "    \"America\": set(df[df['Continents'].str.contains(\"America\")]['Wikipedia_ID'].astype(str)),\n",
    "    \"Europe\": set(df[df['Continents'].str.contains(\"Europe\")]['Wikipedia_ID'].astype(str)),\n",
    "    \"Both\": set(df[df['Continents'].str.contains(\"Both\")]['Wikipedia_ID'].astype(str))\n",
    "}\n",
    "\n",
    "# POS tags for nouns or verbs\n",
    "# !! Ã  completer\n",
    "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "generate_word_cloud(\"America\", noun_tags, \"Nouns in America\", mask_image=americas_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
