{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "from ipywidgets import interact, Dropdown, VBox\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data and Plot Summaries\n",
    "DATASET_FILEPATH = Path(\"data/merged_movie_metadata.csv\")\n",
    "PLOT_SUMMARIES_FILEPATH = Path(\"../MovieSummaries/plot_summaries.txt\")\n",
    "\n",
    "df = pd.read_csv(DATASET_FILEPATH)\n",
    "\n",
    "# Take only unique Wikipedia_ID\n",
    "df = df.drop_duplicates(subset=['Wikipedia_ID'])\n",
    "\n",
    "plot_summaries = {}\n",
    "\n",
    "with open(PLOT_SUMMARIES_FILEPATH, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t', 1)\n",
    "        if len(parts) == 2:\n",
    "            wiki_id, summary = parts\n",
    "            plot_summaries[int(wiki_id)] = summary\n",
    "\n",
    "# Map plot summaries to the DataFrame\n",
    "df['Plot_Summary_Base'] = df['Wikipedia_ID'].map(plot_summaries)\n",
    "\n",
    "\n",
    "# Loading the datasets for the splits\n",
    "dataset_path = \"data/data_p3\"\n",
    "\n",
    "Data_Biggest_America = pd.read_csv(f\"{dataset_path}/Data_Biggest_America.csv\")\n",
    "Data_Biggest_Both = pd.read_csv(f\"{dataset_path}/Data_Biggest_Both.csv\")\n",
    "Data_Biggest_Europe = pd.read_csv(f\"{dataset_path}/Data_Biggest_europe.csv\")\n",
    "Data_Biggest_Gap_diff_Eu_Am = pd.read_csv(f\"{dataset_path}/Data_Biggest_Gap_diff_Eu_Am.csv\")\n",
    "Data_Smallest_Gap_diff_Eu_Am = pd.read_csv(f\"{dataset_path}/Data_Smallest_Gap_diff_Eu_Am.csv\")\n",
    "\n",
    "# Dictionary for the splits\n",
    "dataset_subsets = {\n",
    "    \"Biggest_America\": Data_Biggest_America,\n",
    "    \"Biggest_Both\": Data_Biggest_Both,\n",
    "    \"Biggest_Europe\": Data_Biggest_Europe,\n",
    "    \"Biggest_Gap_diff_Eu_Am\": Data_Biggest_Gap_diff_Eu_Am,\n",
    "    \"Smallest_Gap_diff_Eu_Am\": Data_Smallest_Gap_diff_Eu_Am,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(\"../corenlp_plot_summaries_augmented\")\n",
    "\n",
    "assert folder_path.exists(), \"Must configure the correct path to corenlp_plot_summaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matching folders/files\n",
    "processed_ids = {f.stem.replace(\".xml\", \"\") for f in folder_path.glob(\"*.xml.gz\")}\n",
    "available_ids = set(df[\"Wikipedia_ID\"].astype(str))\n",
    "matching_files = [folder_path/f\"{filename}.xml.gz\" for filename in processed_ids.intersection(available_ids)]\n",
    "\n",
    "print(f\"Number of matching folders/files: {len(matching_files)}\")\n",
    "print(f\"Number of unique Wiki id's: {df['Wikipedia_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_utils import filter_words_by_pos, filter_words_by_pos_ngram, get_sentence_word_metadata\n",
    "\n",
    "# Define Regions\n",
    "region_ids = {\n",
    "    \"America\": set(df[df['Continents'].str.contains(\"America\")]['Wikipedia_ID'].astype(str)),\n",
    "    \"Europe\": set(df[df['Continents'].str.contains(\"Europe\")]['Wikipedia_ID'].astype(str)),\n",
    "    \"Both\": set(df[df['Continents'].str.contains(\"Both\")]['Wikipedia_ID'].astype(str))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've successfully extracted the words we wanted, the next step is to visualize them. To do this, we will use a word cloud visualization to display the most frequent words. In a word cloud, word size represent their frequency of occurence. This gives immediately a grasp of the word occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(region, pos_tags, title, mask_image=None, sample_output=True, ngrams=1, subset_name=None, store_to=None, dpi=300):\n",
    "    \"\"\"Generates a word cloud for a specified region, subset, and POS tags, with optional mask and sample output.\"\"\"\n",
    "    # Filter the region_ids based on the subset\n",
    "    subset_ids = set()\n",
    "    if subset_name and subset_name in dataset_subsets:\n",
    "        subset_ids = set(dataset_subsets[subset_name]['Wikipedia_ID'].astype(str))\n",
    "    \n",
    "    region_subset_ids = region_ids[region].intersection(subset_ids) if subset_name else region_ids[region]\n",
    "    \n",
    "    word_counter = Counter()\n",
    "    for wiki_id in tqdm(region_subset_ids):\n",
    "        tokens_metadata = get_sentence_word_metadata(folder_path, wiki_id)\n",
    "        filtered_words = filter_words_by_pos(tokens_metadata, pos_tags) if ngrams == 1 else filter_words_by_pos_ngram(tokens_metadata, pos_tags, ngrams)\n",
    "        word_counter.update(filtered_words)\n",
    "    \n",
    "    if sample_output:\n",
    "        # Print sample output for verification\n",
    "        print(f\"\\nSample of filtered words for {region} - {title}:\")\n",
    "        print(word_counter.most_common(10))\n",
    "    \n",
    "    # Generate and display the word cloud with mask if provided\n",
    "    wordcloud = WordCloud(width=1200, height=1200, background_color='white', mask=mask_image,\n",
    "                          contour_color='black', contour_width=1).generate_from_frequencies(word_counter)\n",
    "    \n",
    "    # Save or show the word cloud with higher DPI\n",
    "    plt.figure(figsize=(10, 10), dpi=dpi)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    if store_to:\n",
    "        plt.savefig(store_to, dpi=dpi, bbox_inches='tight', pad_inches=0.1)\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tags Groups\n",
    "noun_tags = ['NN', 'NNA', 'NNC', 'NNS', 'NNP', 'NNPS']\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBS', 'VBZ']\n",
    "adjective_tags = ['JJ', 'JJR', 'JJS', 'JJC', 'JJA', 'JJF', 'JJM']\n",
    "\n",
    "# Word Cloud Mask Images\n",
    "americas_mask = np.array(Image.open(\"src/ressource/Location_North_America.png\"))\n",
    "europe_mask = np.array(Image.open(\"src/ressource/Location_Europe.png\"))\n",
    "both_mask = np.array(Image.open(\"src/ressource/Location_Both.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, Dropdown, IntSlider\n",
    "\n",
    "# Dropdown for Region\n",
    "region_dropdown = Dropdown(\n",
    "    options=list(region_ids.keys()),\n",
    "    value=\"Europe\",   # Default\n",
    "    description=\"Region:\"\n",
    ")\n",
    "\n",
    "# Dropdown for Subset\n",
    "subset_dropdown = Dropdown(\n",
    "    options=[None] + list(dataset_subsets.keys()),\n",
    "    value=\"Biggest_Both\",   # Default\n",
    "    description=\"Subset:\"\n",
    ")\n",
    "\n",
    "# Dropdown for POS Tags\n",
    "pos_tags_dropdown = Dropdown(\n",
    "    options=[\"NN\", \"VB\", \"JJ\"],\n",
    "    value=\"NN\",   # Default\n",
    "    description=\"POS Tags:\"\n",
    ")\n",
    "\n",
    "# Slider for N-grams\n",
    "ngrams_slider = IntSlider(\n",
    "    value=1,   # Default\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description=\"N-grams:\"\n",
    ")\n",
    "\n",
    "# Region to Mask Mapping\n",
    "region_to_mask = {\n",
    "    \"America\": americas_mask,\n",
    "    \"Europe\": europe_mask,\n",
    "    \"Both\": both_mask\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = Path(\"docs/assets/img/wordclouds/\")\n",
    "store.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING this took 90min on my laptop.\n",
    "\n",
    "regions = list(region_ids.keys())\n",
    "pos_tags = [\"NN\", \"VB\", \"JJ\"]\n",
    "ngrams = list(range(1, 4))\n",
    "subsets = [None] + list(dataset_subsets.keys())\n",
    "\n",
    "# region = \"Europe\"\n",
    "# pos_tag = \"VB\"\n",
    "# ngram = 3\n",
    "# subset = None\n",
    "\n",
    "for region in regions:\n",
    "    for pos_tag in pos_tags:\n",
    "        for ngram in ngrams:\n",
    "            for subset in subsets:\n",
    "                mask = region_to_mask.get(region, None)\n",
    "                filename = f\"{region}__{subset}__{pos_tag}__{ngram}.jpg\"\n",
    "                generate_word_cloud(region=region,\n",
    "                                    pos_tags=[pos_tag],\n",
    "                                    title=f\"[POS={pos_tag}] - '{subset if subset else 'All available'}' in {region.title()}\",\n",
    "                                    subset_name=subset,\n",
    "                                    sample_output=False,\n",
    "                                    mask_image=mask,\n",
    "                                    ngrams=ngram,\n",
    "                                    store_to=store / filename,\n",
    "                                    dpi = 300)\n",
    "                print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Interactive Word Cloud Function\n",
    "def interactive_word_cloud(region, subset_name, pos_tag, ngrams):\n",
    "    \"\"\"Generates and displays a word cloud based on widget selections\"\"\"\n",
    "    mask_image = region_to_mask.get(region, None)  # Get the correct mask\n",
    "    \n",
    "    generate_word_cloud(\n",
    "        region=region,\n",
    "        pos_tags=[pos_tag],\n",
    "        title=f\"{pos_tag} Word Cloud with {ngrams}-grams\",\n",
    "        mask_image=mask_image,\n",
    "        subset_name=subset_name,\n",
    "        ngrams=ngrams\n",
    "    )\n",
    "\n",
    "# Bind Widgets to Function\n",
    "interact(\n",
    "    interactive_word_cloud,\n",
    "    region=region_dropdown,\n",
    "    subset_name=subset_dropdown,\n",
    "    pos_tag=pos_tags_dropdown,\n",
    "    ngrams=ngrams_slider \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
